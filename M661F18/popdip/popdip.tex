\documentclass[11pt]{article}
%prepared in AMSLaTeX, under LaTeX2e
\addtolength{\oddsidemargin}{-.75in} 
\addtolength{\evensidemargin}{-.75in}
\addtolength{\topmargin}{-.4in}
\addtolength{\textwidth}{1.4in}
\addtolength{\textheight}{1.0in}

\renewcommand{\baselinestretch}{1.075}

\usepackage{verbatim,fancyvrb}

\usepackage{palatino,amsmath,amssymb,amsthm}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}

\newtheorem*{thm}{Theorem}
\newtheorem*{defn}{Definition}
\newtheorem*{example}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{remark}{Remark}

\newcommand{\mtt}{\texttt}
\usepackage{alltt,xspace}
\newcommand{\mfile}[1]
{\medskip\begin{quote}\scriptsize \begin{alltt}\input{#1.m}\end{alltt} \normalsize\end{quote}\medskip}

%\usepackage[final]{graphicx}

\usepackage[pdftex, colorlinks=true, plainpages=false, linkcolor=blue, citecolor=red, urlcolor=blue]{hyperref}

% macros
\newcommand{\bc}{\mathbf{c}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lam}{\lambda}
\newcommand{\lap}{\triangle}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}

%\renewcommand{\det}{\operatorname{det}}
\newcommand{\onull}{\operatorname{null}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\range}{\operatorname{range}}

\newcommand{\prob}[1]{\bigskip\noindent\textbf{#1.}\quad }
\newcommand{\exer}[2]{\prob{Exercise #2 in Lecture #1}}

\newcommand{\pts}[1]{(\emph{#1 pts}) }
\newcommand{\epart}[1]{\medskip\noindent\textbf{(#1)}\quad }
\newcommand{\ppart}[1]{\,\textbf{(#1)}\quad }

\newcommand{\Julia}{\textsc{Julia}\xspace}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\Python}{\textsc{Python}\xspace}

\DefineVerbatimEnvironment{mVerb}{Verbatim}{numbersep=2mm,
frame=lines,framerule=0.1mm,framesep=2mm,xleftmargin=4mm,fontsize=\footnotesize}

\newcommand{\ema}{\emach}
\newcommand{\emach}{\eps_{\!_{\text{m}}}}

\title{POPDIP: \\ the POsitive-variables Primal-Dual Interior Point method}
\author{Ed Bueler}
\date{\today}

\begin{document}
\maketitle

%\abstract{This is not research, but it is a new algorithm for me.  It is a version of the primal dual interior point algorithm in \cite{GrivaNashSofer2009}.}
\begin{abstract}
POPDIP is a version of the primal dual interior point algorithm in \cite{GrivaNashSofer2009}; see section 16.7 and Algorithm 16.1.  (``POPDIP'' is just a name I made up; it is not in common use.)  It minimizes a smooth nonlinear function subject to the constraints that all the variables are nonnegative.

These short notes are not research!  Indeed this algorithm is simply a special case of a well-known algorithm.  However, it is new to me so I am documenting it fully.
\end{abstract}

\thispagestyle{empty}

\bigskip
Consider the nonlinear optimization problem with positivity constraints on the variables:
\begin{equation}
\begin{matrix}
\text{minimize} \qquad & f(x) \\
\text{subject to} \qquad & x \ge 0
\end{matrix} \label{minproblem}
\end{equation}
Here $f:\RR^n\to\RR^n$ is a smooth function and ``$x\ge 0$'' means that each entry of $x\in\RR^n$ is nonnegative.  Clearly the feasible set of \eqref{minproblem} is $S = \{x\in \RR^n\,:\,x\ge 0\}$.

Let $\mu>0$.  If $x$ is in the interior of $S$ then the following ``barrier function'' is finite:
\begin{equation}
\beta_\mu = f(x) - \mu \sum_{i=1}^n \ln x_i \label{barrierfunction}
\end{equation}
The first-order necessary conditions for the unconstrained problem of minimizing $\beta_\mu$, namely $\grad \beta_\mu(x)=0$ for $x$ in the interior of $S$, are
\begin{align}
x &> 0 \label{firstorderbarrier} \\
\grad f(x) - \mu \sum_{i=1}^n x_i^{-1} e_i &= 0 \notag
\end{align}
Here $\{e_1,\dots,e_n\}$ is the standard basis of $\RR^n$.

Conditions \eqref{firstorderbarrier} can be reformulated by defining additional variables $\lambda_i = \mu / x_i$ where $\lambda\in\RR^n$.  Note that $\lambda>0$ if and only if $x>0$ because $\lambda_i x_i = \mu > 0$.  Now \eqref{firstorderbarrier} is precisely equivalent to the following nonlinear system of equations and inequalities:
\begin{align}
x &\ge 0 \label{firstordersystem} \\
\lambda &\ge 0 \notag \\
\grad f(x) - \sum_{i=1}^n \lambda_i e_i &= 0 \notag \\
\lambda_i x_i &= \mu, \qquad i=1,\dots,n \notag
\end{align}
Note that, because of the last condition, both $x$ and $\lambda$ are positive and thus in the interiors of their respective feasible sets.

Conditions \eqref{firstordersystem} are related to an obvious definition of a Lagrangian function for \eqref{minproblem}, namely
    $$\mathcal{L}(x,\lambda) = f(x) - \sum_{i=1}^n \lambda_i x_i,$$
so that the third line of \eqref{firstordersystem} is the statement that $\grad_x \mathcal{L}(x,\lambda)=0$.  However, the whole system \eqref{firstordersystem} says more than the statement of that the unconstrained Lagrangian has a stationary point.  Namely, there is an additional connection between the variables ($\lambda_i x_i = \mu$) and there are additional nonnegativity constraints ($x\ge 0$ and $\lambda\ge 0$).

Algorithm 16.1 in section 16.7 of \cite{GrivaNashSofer2009}, a primal-dual interior point algorithm, applies to \eqref{minproblem}.  POPDIP is the resulting simplified algorithm which uses the fact that $g_i(x)=x_i$.  It computes approximate solutions to \eqref{firstordersystem} for a sequence $\mu=\mu^{(k)} \to 0$.  In that limit the exact solution solves \eqref{firstordersystem} with $\mu$ replaced by zero, and these conditions are the Karush-Kuhn-Tucker conditions for \eqref{minproblem}; see Lemma 14.8 and Theorem 14.18 in \cite{GrivaNashSofer2009}.

Each step of the algorithm is a Newton step for the nonlinear system
\begin{align}
\grad f(x) - \sum_{i=1}^n \lambda_i e_i &= 0 \label{equalitysystem} \\
\lambda_i x_i &= \mu, \qquad i=1,\dots,n \notag
\end{align}
in the variables $x$ and $\lambda$.  To describe the Newton step let $x=x^{(k)}+\Delta x$, $\lambda=\lambda^{(k)}+\Delta\lambda$, and $\mu=\mu^{(k)}$.  (The current iterate is $(x^{(k)},\lambda^{(k)})$ generally does not solve \eqref{equalitysystem}.  The unknowns in the Newton step are the search direction $p=(\Delta x,\Delta \lambda)$.)  Substituting these into \eqref{equalitysystem} and expanding to first order gives
\begin{align}
\grad f(x^{(k)}) - \sum_{i=1}^n \lambda_i^{(k)} e_i + \grad^2 f(x^{(k)}) \Delta x - \sum_{i=1}^n (\Delta \lambda)_i e_i &= 0 \label{prenewtonstep} \\
\lambda_i^{(k)} x_i^{(k)} + x_i^{(k)} (\Delta\lambda)_i + \lambda_i^{(k)} (\Delta x)_i &= \mu^{(k)}, \qquad i=1,\dots,n \notag
\end{align}
Rearranging as a linear block system for the search direction, and suppressing the ``$(k)$'' superscript on the current iterate gives the Newton step equations
\begin{equation}
\begin{bmatrix}
\grad^2 f(x) & - I \\
\Lambda & X
\end{bmatrix}
\begin{bmatrix}
\Delta x \\
\Delta \lambda
\end{bmatrix}
=
\begin{bmatrix}
-\grad f(x) + \sum_{i=1}^n \lambda_i e_i \\
-\Lambda x + \mu^{(k)} e
\end{bmatrix}
 \label{newtonstep}
\end{equation}
where $I$ is the $n\times n$ identity matrix, $X$ is an $n\times n$ diagonal matrix with the entries of the vector $x$ on the diagonal, and similarly $\Lambda$ is an $n\times n$ diagonal matrix with the entries of the vector $\lambda$ on the diagonal.

Equation \eqref{newtonstep} can be symmetrized by multiplying the second half of the equations by $-\Lambda^{-1}$:
\begin{equation}
\begin{bmatrix}
\grad^2 f(x) & - I \\
-I & - \Lambda^{-1} X
\end{bmatrix}
\begin{bmatrix}
\Delta x \\
\Delta \lambda
\end{bmatrix}
=
\begin{bmatrix}
-\grad f(x) + \sum_{i=1}^n \lambda_i e_i \\
x - \mu^{(k)} \Lambda^{-1} e
\end{bmatrix}
 \label{symmnewtonstep}
\end{equation}
(Note that It is not clear to me at this time whether solving \eqref{newtonstep} or \eqref{symmnewtonstep} makes a better algorithm.

In Algorithm 16.1 the computation of the Newton search direction is followed by separate line searches in $x$ and in $\lambda$.  The goals of these line searches is only to maintain the nonnegativity requirements ($g_i(x)\ge 0$ and $\lambda_i\ge 0$, in general) and not to get sufficient decrease of $f(x)$.  However, in our case, because of the linearity of the constraint functions, namely of the functions $g_i(x)=x_i$, ratio tests can be applied in each line search to bound the step sizes $\alpha$.  Thus no back-tracking line search is required.\footnote{This algorithm design suggests possible improvements.  Back-tracking line search is, of course, appropriate as a globalization for unconstrained optimization.}


\bigskip
\noindent \textsc{Algorithm. the POsitive-variables Primal-Dual Interior Point method.}
\begin{quote}
\begin{itemize}
\item[\emph{input}]  FIXME
\item[\emph{output}]  FIXME
\end{itemize}
\renewcommand{\labelenumi}{\arabic{enumi}.}
\begin{enumerate}
\item FIXME
\item For $k=0,1,2,\dots$
    \renewcommand{\labelenumii}{(\alph{enumii})}
    \begin{enumerate}
    \item FIXME
    \end{enumerate}
\end{enumerate}
\end{quote}


\bibliography{popdip}
\bibliographystyle{siam}

\end{document}

